{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9dc3ee-1765-4d59-af94-b0e38286635e",
   "metadata": {},
   "source": [
    "# Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc60f26-e3e9-4af9-9adb-b2ce1392a4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from swifter import swifter\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import awswrangler as wr\n",
    "from sklearn.compose import ColumnTransformer,make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,FunctionTransformer, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import TargetEncoder\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import model_selection, metrics\n",
    "import sagemaker\n",
    "import awscli\n",
    "import os\n",
    "import s3fs\n",
    "import joblib\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "region = os.getenv('region')\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=False, key=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "def data_description(df):\n",
    "    print('Variables:\\n\\n{}'.format(df.dtypes), end='\\n\\n')\n",
    "    print('Number of rows {}'.format(df.shape[0]), end='\\n\\n')\n",
    "    print('Number of columns {}'.format(df.shape[1]), end='\\n\\n')\n",
    "    print('NA analysis'.format(end='\\n'))\n",
    "    for i in df.columns:\n",
    "        print('column {}: {} {}'.format(i,df[i].isna().any(), df[i].isna().sum()))\n",
    "\n",
    "def consult_table_athena(database, table):\n",
    "    wr.config.aws_profile = 'default'\n",
    "    wr.config.region = 'us-east-1'\n",
    "\n",
    "    query = f\"SELECT * FROM {database}.{table}\"\n",
    "\n",
    "    df = wr.athena.read_sql_query(query, database=database)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def unique_values_columns(df):\n",
    "    \"\"\"\n",
    "    Display unique values for each object (or string) column in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary with column names as keys and unique values as lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out only object or string type columns\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Get unique values for each object column\n",
    "    unique_values = {col: df[col].unique().tolist() for col in object_cols}\n",
    "    \n",
    "    return unique_values\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8ab5a-a71e-4076-8875-3f5f47c74b5a",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a20bf5-8f08-4747-8961-070440692323",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading raw data from athena\n",
    "# Balancing classes\n",
    "# Separate train and test data\n",
    "\n",
    "\n",
    "# DATABASE = 'respiratory_db'\n",
    "# TABLE = 'table_respiratory_traintrain_data'\n",
    "# df = consult_table_athena(DATABASE, TABLE)\n",
    "\n",
    "# df.to_parquet('data.parquet', index=False)\n",
    "\n",
    "# df_raw['classi_fin'].value_counts().sort_index(ascending=True)\n",
    "# class1 = df_raw[df_raw['classi_fin'] == 1].sample(31437, random_state=42)\n",
    "# class2 = df_raw[df_raw['classi_fin'] == 2]\n",
    "# class3 = df_raw[df_raw['classi_fin'] == 3]\n",
    "# class4 = df_raw[df_raw['classi_fin'] == 4].sample(31437, random_state=42)\n",
    "# class5 = df_raw[df_raw['classi_fin'] == 5].sample(31437, random_state=42)\n",
    "# df_raw = pd.concat([class1, class2, class3, class4, class5], ignore_index=True)\n",
    "\n",
    "# train, test = train_test_split(df_raw, test_size=0.15, random_state=42)\n",
    "\n",
    "# train.to_parquet('train.parquet', index=False)\n",
    "# test.to_parquet('test.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784b93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f16a2-bbd8-4515-95fe-457260c7772e",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8a538-34b2-4534-8f51-8c9d9e720c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94311d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32826bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbc1a5-8215-48a2-926a-8afa4fe7ae25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb71955-7224-4d67-91a2-d70132034814",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Cleaning / NA analysis / Outliers analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc225c97-4d6e-4d4e-af3f-e4c7909c2f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sexo have Male, F and I, lets take off the I value has there are just 188 lines.\n",
    "df = df.loc[df['cs_sexo'] != 'I']\n",
    "df['cs_sexo'] = df['cs_sexo'].astype('category')\n",
    "\n",
    "df = df.drop('delta_uti', axis=1)\n",
    "\n",
    "# Negative ages are excluded\n",
    "df = df[~df['nu_idade_n'] <= 0]\n",
    "\n",
    "# Remove demographic categories\n",
    "\n",
    "df = df.drop(\n",
    "    [\n",
    "    'sg_uf_not',\n",
    "    'id_regiona',\n",
    "    'co_regiona',\n",
    "    'id_municip',\n",
    "    'co_mun_not',\n",
    "    'sg_uf',\n",
    "    'cod_idade',\n",
    "    'cs_escol_n'\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "df['tp_amostra'].replace(df['tp_amostra'].max(),df['tp_amostra'].mode()[0], inplace=True)\n",
    "df['dor_abd'].replace(df['dor_abd'].max(),df['dor_abd'].mode()[0], inplace=True)\n",
    "df['perd_olft'].replace(df['perd_olft'].max(),df['perd_olft'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb6950-d7a9-4192-aea1-c3f1af903b32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "float_cols = df.select_dtypes(['float16','float32','int64']).columns\n",
    "df[float_cols] = df[float_cols].swifter.apply(\n",
    "    lambda x: x.fillna(x.mode()[0])\n",
    ")\n",
    "\n",
    "df[float_cols] = df[float_cols].astype('int8')\n",
    "\n",
    "int_cols = df.select_dtypes(['int64','int8']).drop(['sem_not','sem_pri','nu_idade_n'], axis=1).columns\n",
    "df[int_cols] = df[int_cols].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f17329",
   "metadata": {},
   "source": [
    "### Feature selection to remove unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Excluding text columns\n",
    "# predictors = df.drop('classi_fin', axis=1)\n",
    "# predictors = predictors.drop(predictors.select_dtypes(['object','string']), axis=1)\n",
    "# response = df['classi_fin']\n",
    "\n",
    "# X_train, X_val, y_train,y_val = train_test_split(\n",
    "#     predictors, response, random_state=50, stratify=response\n",
    "# )\n",
    "\n",
    "# numerical_cols = predictors.select_dtypes(['int8']).columns.tolist()\n",
    "# categorical_cols = predictors.select_dtypes(['category']).columns.tolist()\n",
    "\n",
    "# categorical_imputer = ColumnTransformer(\n",
    "#     [('cat_imputer', SimpleImputer(strategy='most_frequent'), categorical_cols)],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "\n",
    "\n",
    "# X_train_transformed = pd.DataFrame(\n",
    "#     categorical_imputer.fit_transform(X_train),\n",
    "#     columns=categorical_cols,\n",
    "#     index=X_train.index\n",
    "# )\n",
    "# X_train_transformed = pd.concat([X_train_transformed,X_train[numerical_cols]],axis=1)\n",
    "# X_train_transformed[categorical_cols] = X_train_transformed[categorical_cols].astype('category')\n",
    "\n",
    "# X_val_transformed = pd.DataFrame(\n",
    "#     categorical_imputer.transform(X_val),\n",
    "#     columns=categorical_cols,\n",
    "#     index=X_val.index\n",
    "# )\n",
    "# X_val_transformed = pd.concat([X_val_transformed,X_val[numerical_cols]],axis=1)\n",
    "# X_val_transformed[categorical_cols] = X_val_transformed[categorical_cols].astype('category')\n",
    "\n",
    "# categorical_transformer = Pipeline(\n",
    "#     steps=[\n",
    "#         ('encoder', CountFrequencyEncoder(encoding_method='frequency'))\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# numerical_transformer = Pipeline(\n",
    "#     steps=[\n",
    "#         ('encoder', MinMaxScaler())\n",
    "#     ]    \n",
    "# )\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     [\n",
    "#         ('cat', categorical_transformer, categorical_cols),\n",
    "#         ('num', numerical_transformer, numerical_cols)\n",
    "#     ]   \n",
    "# )\n",
    "\n",
    "# X_train_transformed = pd.DataFrame(\n",
    "#     preprocessor.fit_transform(X_train),\n",
    "#     columns=predictors.columns,\n",
    "#     index=X_train_transformed.index\n",
    "# )\n",
    "\n",
    "# X_val_transformed = pd.DataFrame(\n",
    "#     preprocessor.transform(X_val),\n",
    "#     columns=predictors.columns,\n",
    "#     index=X_val_transformed.index\n",
    "# )\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "# y_train = encoder.fit_transform(y_train)\n",
    "# y_val = encoder.transform(y_val)\n",
    "\n",
    "# classifiers = [\n",
    "#     RandomForestClassifier(n_jobs=-1, random_state=42, class_weight='balanced_subsample'),\n",
    "#     XGBClassifier(n_jobs=-1, random_state=42,objective='multi:softax', max_delta_step=1),\n",
    "#     lgbm.LGBMClassifier(n_jobs=-1,  random_state=42, class_weight='balanced')\n",
    "# ]\n",
    "\n",
    "# for classifier in classifiers:\n",
    "#     pipeline_1 = Pipeline(\n",
    "#         steps= [\n",
    "#         ('feature_selection', SelectFromModel(estimator=classifier))\n",
    "#         ]\n",
    "#     )\n",
    "#     pipeline_2 = Pipeline(\n",
    "#         steps= [\n",
    "#         ('feature_selection', RFE(estimator=classifier))\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     permutation_score = permutation_importance(\n",
    "#         classifier.fit(X_train_transformed,y_train), X_val_transformed, y_val,\n",
    "#         random_state=42, scoring='f1_weighted', n_repeats=10\n",
    "#     )\n",
    "\n",
    "#     importance = pd.DataFrame(\n",
    "#         {'features':X_train_transformed.columns, \n",
    "#         'f1_weighted':permutation_score['importances_mean']}).sort_values(by='f1_weighted', ascending=False\n",
    "#     )\n",
    "\n",
    "#     print(\n",
    "#         'model: {} \\n features selected based on feature importance:{} \\n\\n'.format(pipeline_1['feature_selection'].estimator,\n",
    "#         pipeline_1.fit(X_train_transformed,y_train).get_feature_names_out(input_features=None)) \n",
    "#         )\n",
    "#     print(\n",
    "#         'model: {} \\n features_selected based on RFE:{} \\n\\n'.format(pipeline_2['feature_selection'].estimator,\n",
    "#         pipeline_2.fit(X_train_transformed,y_train).get_feature_names_out(input_features=None))\n",
    "#         )\n",
    "#     print(importance, '\\n\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "#Features choosen to continue\n",
    "feature_selected = [\n",
    "    'id', 'sem_pri', 'nu_idade_n', 'saturacao', 'antiviral',\n",
    "    'tp_antivir', 'hospital', 'dose_2_cov', 'dose_ref', 'classi_fin',\n",
    "    'fnt_in_cov', 'uti','raiox_res', 'dor_abd', 'perd_olft', 'tomo_res',\n",
    "    'cs_raca', 'cs_zona', 'perd_pala', 'dose_1_cov','vacina_cov'\n",
    "]\n",
    "\n",
    "df = df[feature_selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f5ad3",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac111f",
   "metadata": {},
   "source": [
    "# Response analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df['classi_fin'].apply(\n",
    "    lambda x: 'SARS by influenza' if x == 1\n",
    "    else 'SARS by other respiratory virus' if x == 2\n",
    "    else 'SARS by another etiological agent' if x == 3\n",
    "    else 'unspecified SARS' if x == 4\n",
    "    else 'SARS by covid-19'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a7fc3",
   "metadata": {},
   "source": [
    "# Data preparation and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 9 values to mode of each column\n",
    "df_category = (\n",
    "    df.select_dtypes(include='category')\n",
    "    .columns.to_list()\n",
    ")\n",
    "column_modes = df[df_category].mode().iloc[0]\n",
    "for col_name in df_category:\n",
    "    df[col_name] = df[col_name].replace(9, column_modes[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['id','classi_fin'], axis=1)\n",
    "y = df['classi_fin']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = pd.Series(encoder.fit_transform(y), index=y.index)\n",
    "\n",
    "\n",
    "numerical_cols = X.select_dtypes(['int8']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(['category']).columns.tolist()\n",
    "columns_name = categorical_cols + numerical_cols\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "role = \"arn:aws:iam::513734873949:role/FULL_SAGEMAKER\"\n",
    "bucket = 'sagemaker-traintest-respiratory-classification'\n",
    "# prefix1 = 'train'\n",
    "# prefix2 = 'val'\n",
    "prefixprep = 'preprocessor'\n",
    "prefixestimat = 'estimator'\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'eta': sagemaker.tuner.CategoricalParameter([0.1, 0.5]),\n",
    "    'max_depth': sagemaker.tuner.CategoricalParameter([2, 9]),\n",
    "    'gamma': sagemaker.tuner.CategoricalParameter([3, 10]),\n",
    "    'min_child_weight': sagemaker.tuner.CategoricalParameter([8, 15]),\n",
    "    'subsample': sagemaker.tuner.CategoricalParameter([0.5, 0.7])\n",
    "}\n",
    "\n",
    "cv_metric = []\n",
    "cv_best_estimator = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            ('encoder', CountFrequencyEncoder(encoding_method='frequency'))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    numerical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            ('encoder', MinMaxScaler())\n",
    "        ]    \n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "            ('num', numerical_transformer, numerical_cols)\n",
    "        ]   \n",
    "    )\n",
    "    \n",
    "    X_train = pd.DataFrame(\n",
    "        preprocessor.fit_transform(X_train),\n",
    "        columns=columns_name,\n",
    "        index=X_train.index\n",
    "    )\n",
    "\n",
    "    X_val = pd.DataFrame(\n",
    "        preprocessor.transform(X_val),\n",
    "        columns=columns_name,\n",
    "        index=X_val.index\n",
    "    )\n",
    "    \n",
    "    # Sending to S3\n",
    "    train_data = pd.concat([y_train,X_train], axis=1)\n",
    "    val_data = pd.concat([y_val,X_val], axis=1)\n",
    "    train_data = train_data.rename(columns={0:'classi_fin'})\n",
    "    val_data = val_data.rename(columns={0:'classi_fin'})\n",
    "\n",
    "    prefix1 = 'train_fold_{}'.format(i)\n",
    "    prefix2 = 'val_fold_{}'.format(i)\n",
    "    \n",
    "    train_data.to_csv('train_processed_fold_{}.csv'.format(i), header=False, index=False)\n",
    "    val_data.to_csv('val_processed_fold_{}.csv'.format(i), header=False, index=False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    train_data_path = sagemaker_session.upload_data(path='train_processed_fold_{}.csv'.format(i), bucket=bucket, key_prefix=prefix1)\n",
    "    val_data_path = sagemaker_session.upload_data(path='val_processed_fold_{}.csv'.format(i), bucket=bucket, key_prefix=prefix2)\n",
    "\n",
    "    s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/'.format(bucket, prefix1), content_type='csv')\n",
    "    s3_input_val = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/'.format(bucket, prefix2), content_type='csv')\n",
    "    \n",
    "    \n",
    "    container = sagemaker.image_uris.retrieve(region=sagemaker_session.boto_region_name, framework='xgboost', version='1.7-1')\n",
    "\n",
    "\n",
    "    estimator = sagemaker.estimator.Estimator(\n",
    "        container,\n",
    "        role,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.c5.2xlarge',\n",
    "        output_path='s3://{}/{}/output_fold_{}'.format(bucket, prefixestimat, i),\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        hyperparameters= {\n",
    "            'num_round': 100,\n",
    "            'num_class': 5,\n",
    "            'objective':'multi:softprob'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=estimator, \n",
    "    objective_metric_name='validation:mlogloss', \n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=8,\n",
    "    max_parallel_jobs=3,\n",
    "    objective_type='Minimize'\n",
    "    )\n",
    "\n",
    "    tuner.fit({'train': s3_input_train, 'validation': s3_input_val})\n",
    "    \n",
    "    tuner.wait()\n",
    "    \n",
    "    best_training_job_name = tuner.best_training_job()\n",
    "    \n",
    "    sagemaker_client = session.client('sagemaker')\n",
    "    best_job_details = sagemaker_client.describe_training_job(TrainingJobName=best_training_job_name)\n",
    "    best_hyperparameters = best_job_details['HyperParameters']\n",
    "    \n",
    "    best_estimator = sagemaker.estimator.Estimator.attach(best_training_job_name)\n",
    "    predictor = best_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "    with s3.open('s3://{}/{}/val_processed_fold_{}.csv'.format(bucket, prefix2, i), 'rb') as f:\n",
    "        X_val = pd.read_csv(f)\n",
    "    \n",
    "    \n",
    "    csv_buffer = io.StringIO()\n",
    "    X_val.to_csv(csv_buffer, header=False, index=False)\n",
    "    csv_payload = csv_buffer.getvalue().encode('utf-8')\n",
    "    predictions = predictor.predict(csv_payload)\n",
    "    metric = metrics.log_loss(y_val, predictions)\n",
    "    cv_best_estimator.append(best_hyperparameters)\n",
    "    cv_metric.append(metric)\n",
    "    predictor.delete_endpoint()\n",
    "    \n",
    "    print(cv_best_estimator)\n",
    "    print(cv_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = X.select_dtypes(['int8']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(['category']).columns.tolist()\n",
    "columns_name = categorical_cols + numerical_cols\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('encoder', CountFrequencyEncoder(encoding_method='frequency'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('encoder', MinMaxScaler())\n",
    "    ]    \n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        ('num', numerical_transformer, numerical_cols)\n",
    "    ]   \n",
    ")\n",
    "\n",
    "X_train = pd.DataFrame(\n",
    "    preprocessor.fit_transform(X_train),\n",
    "    columns=columns_name,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_val = pd.DataFrame(\n",
    "    preprocessor.transform(X_val),\n",
    "    columns=columns_name,\n",
    "    index=X_val.index\n",
    ")\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_train = pd.Series(y_train, index=train_index)\n",
    "y_val = encoder.transform(y_val)\n",
    "y_val = pd.Series(y_val, index=val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ea890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new bucket for processed data\n",
    "bucket = 'sagemaker-traintest-respiratory-classification'\n",
    "prefix1 = 'train'\n",
    "prefix2 = 'test'\n",
    "prefixestimat = 'estimator'\n",
    "\n",
    "# # Sending to S3\n",
    "# train_data = pd.concat([y_train,X_train], axis=1)\n",
    "# val_data = pd.concat([y_val,X_val], axis=1)\n",
    "# train_data = train_data.rename(columns={0:'classi_fin'})\n",
    "# val_data = val_data.rename(columns={0:'classi_fin'})\n",
    "\n",
    "# train_data.to_csv('train_processed.csv', header=False, index=False)\n",
    "# val_data.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# train_data_path = sagemaker_session.upload_data(path='train_processed.csv', bucket=bucket, key_prefix=prefix1)\n",
    "# val_data_path = sagemaker_session.upload_data(path='validation.csv', bucket=bucket, key_prefix=prefix2)\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/'.format(bucket, prefix1), content_type='csv')\n",
    "s3_input_val = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/'.format(bucket, prefix2), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f47563",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"arn:aws:iam::513734873949:role/FULL_SAGEMAKER\"\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(region=sagemaker_session.boto_region_name, framework='xgboost', version='1.7-1')\n",
    "\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(container,\n",
    "                                          role,\n",
    "                                          instance_count=1,\n",
    "                                          instance_type='ml.m4.xlarge',\n",
    "                                          output_path='s3://{}/{}/output'.format(bucket, prefixestimat),\n",
    "                                          sagemaker_session=sagemaker_session)\n",
    "\n",
    "estimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.t2.large')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "respiratory_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
